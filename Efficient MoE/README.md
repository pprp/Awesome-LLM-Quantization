|Title & Author & Link|Introduction| Summary|
|------|------|------|
|[XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection](https://arxiv.org/abs/2403.18926) </br> Yuanhang Yang, Shiyi Qi, Wenchao Gu, Chaozheng Wang, Cuiyun Gao, Zenglin Xu||This paper proposes enhanced sparse MoE models that have the small expert and the threshold-based router.|
|[Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/abs/2404.01365) </br> Harry Dong, Beidi Chen, Yuejie Chi||This paper proposes a token load balance schedule during the prompt phase, exploiting the flocking phenomenon. Flocking originally describes a group of birds flying together. Metaphorically, some token patterns could trigger many neuron activations at once. The new load balance matches the token pattern with experts that will highly likely flock.|
|[Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts](https://arxiv.org/abs/2404.05019) </br> Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang||This paper focuses on reducing Parallelism communication cost on MoE, introducing two models DoubleGating MoE(DGMoE) and Shortcut MoE(ScMoE).|
|[SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts](https://arxiv.org/abs/2404.05089) </br> Alexandre Muzio, Alex Sun, Churan He||This paper proposes a Two-Stage Framework containing expert weight pruning and entropy-based regularization fine-tuning. The pruning is based on counting either hard count on the number of activated experts or soft count based on softmax function and its scale can be both layer-wise and global.|
|[Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models](https://arxiv.org/abs/2404.05567) </br> Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, Rameswar Panda||This paper proposes a hybrid dense training and sparse inference framework for MoE models. New sparse inference needs new mutual information loss as criteria of the routing function. Self-attention layer is replaced by Group-query attention. This paper's analysis reveals that traditional sparse training methods do not optimize the Mixture of expert models.|
|MLsys24 </br> [Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping](https://arxiv.org/abs/2404.19429) Chenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, Yida Wang||This paper focuses on computational graph optimization at the compiler level, using RAF an extension of Apache TVM.|
|ICML25 </br> [A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts](https://arxiv.org/pdf/2405.16646) </br> Mohammed Nowaz Rabbani Chowdhury, Meng Wang, Kaoutar El Maghraoui, Naigang Wang, Pin-Yu Chen, Christopher Carothers||The contribution of this work is primarily pruning experts' large change of the router's $l_{2}$ norm. Two lemmas may be worth mentioning: important experts become more specialized and unimportant experts stay unimportant. Within a 1% accuracy error, 50% of experts equivalent to 35% of the whole model can be pruned.|
|[Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models](https://arxiv.org/abs/2405.14297) </br> Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Tao Lin||This paper presents a new gating method and adaptive process of expert number determination. Instead of the softmax function, the sigmoid function is used in the gating function. Inspired by the caching replacement technique, the Least Recently Used Cache-like method decides the number of experts based on whether the token can be assigned and the expert has a task to process. 7.5x end-to-end inference speedup was achieved.|
|[MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models](https://arxiv.org/abs/2405.18832) </br> Taehyun Kim, Kwanseok Choi, Youngmock Cho, Jaehoon Cho, Hyuk-Jae Lee, Jaewoong Sim||MONDE is a near-data computing architecture that optimizes a Mixture of Expert processes by utilizing a Computer Express link-based Memory Device that can extend terabytes of memory to the CPU.|
